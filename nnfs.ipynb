{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec25fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initializing a random generator with seed \n",
    "rng = np.random.default_rng(seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbda9c",
   "metadata": {},
   "source": [
    "### Activation Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0ee027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "  \n",
    "# # get the image\n",
    "# Image(data=r\"C:\\Users\\rachi\\DL\\ANN\\Activations.png\",width=500, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd44655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.where(z>=0,z,0)\n",
    "\n",
    "def grad_relu(z):\n",
    "    return np.where(z>=0,1,0)\n",
    "\n",
    "def prelu(z):\n",
    "    return np.where(z>=0,z,0.01*z)\n",
    "\n",
    "def grad_prelu(z):\n",
    "    return np.where(z>=0,1,0.01)\n",
    "\n",
    "def erelu(z):\n",
    "    return np.where(z>=0,z,0.05*np.exp(z)-1)\n",
    "\n",
    "def grad_erelu(z):\n",
    "    return np.where(z>=0,1,erelu(z)+0.05)\n",
    "\n",
    "def tanh(z):\n",
    "    return (2/(1+np.exp(-2*z)))-1\n",
    "\n",
    "def grad_tanh(z):\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "# define dictionary of activation functions and their derivatives\n",
    "act_func = {'sigmoid':sigmoid,'relu':relu,'tanh':tanh,'prelu':prelu,'erelu':erelu}\n",
    "grad_act_func = {'sigmoid':grad_sigmoid,'relu':grad_relu,'tanh':grad_tanh,'prelu':prelu,'erelu':erelu}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88163460",
   "metadata": {},
   "source": [
    "Output layer either uses identity(regression) or softmax(classification) activation\n",
    "\n",
    "The input to the softmax function will always be a matrix of size n X k, where k is the number of classes. Since we need a probability distribution for each data-point, the softmax will be computed row-wise. As softmax is used in last layer for classification problem\n",
    "\n",
    "np keepdims: https://stackoverflow.com/questions/39441517/in-numpy-sum-there-is-parameter-called-keepdims-what-does-it-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7134300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def softmax(z):\n",
    "    assert z.ndim == 2\n",
    "    \n",
    "#     prevent overflow\n",
    "    z -= z.max(axis=1,keepdims=True)\n",
    "    \n",
    "    prob = np.exp(z)/(np.exp(z).sum(axis=1,keepdims=True))\n",
    "    return prob\n",
    "\n",
    "output_layer = {'identity':identity,'softmax':softmax}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38ce14",
   "metadata": {},
   "source": [
    "#### Loss functions:\n",
    "https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9#:~:text=A%20loss%20function%20is%20a,the%20predicted%20and%20target%20outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b18220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y,yp):\n",
    "    return np.sum((yp-y)*(yp-y))/(y.shape[0])\n",
    "\n",
    "def r2_score(y,yp):\n",
    "    mean = np.mean(y)\n",
    "    return 1 - ((np.sum((yp-y)*(yp-y)))/(np.sum((mean-y)*(mean-y))))\n",
    "\n",
    "def cce_loss(y,yp):\n",
    "    return -1*np.sum(y*np.log(yp))/(y.shape[0])\n",
    "\n",
    "loss_func = {'mse':mse_loss,'cce':cce_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b507dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of parameters\n",
    "\n",
    "def count_params(layers):\n",
    "    total = 0\n",
    "    for i in range(1,len(layers)):\n",
    "        numw = layers[i-1]*layers[i]\n",
    "        numb = layers[i]\n",
    "        total += (numw + numb)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5129bf",
   "metadata": {},
   "source": [
    "#### Parameter initialization:\n",
    "\n",
    "Weight vector **W** is of same size as layers, where **W[ i ]** stores a matrix containing weights connecting layer i-1 to i.\n",
    "Hence size of matrix at **W[ i ]** is layers[i-1] * layers[i]. For Bias vector **b** it is of same size of layers and **b[ i ]** contains contains a array of biases for layer i. First element of **W & b** will always be None.\n",
    "\n",
    "To make the gradient descent update simpler, it will be useful to have a **master vector, $\\theta$,** that has a reference to all the parameters in the network.<br>\n",
    "We will do the same for the gradients  $\\theta^{(g)}$. So, whenever $\\theta$ is updated, the weights W, will also be updated and vice-versa.\n",
    "\n",
    "\n",
    "One way to do this is to first start with the master vector and then **reshape chunks of it into the dimensions of a weight matrix.** Reshaping an\n",
    "array usually returns a view of an array and not a copy. To understand this function better, refer to NumPy's documentation on \"Copies and\n",
    "Views*: https://numpy.org/doc/stable/user/basics.copies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff667e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers):\n",
    "    num_params = count_params(layers)\n",
    "    \n",
    "    w = [None for i in range(len(layers))]\n",
    "    b = [None for i in range(len(layers))]\n",
    "    gw = [None for i in range(len(layers))]\n",
    "    gb = [None for i in range(len(layers))]\n",
    "    \n",
    "    theta = rng.standard_normal(num_params)\n",
    "    gtheta = rng.standard_normal(num_params)\n",
    "    \n",
    "#     (start, end) specify the portion of the theta\n",
    "#     that corresponds to the parameter, W_1 or b_1\n",
    "    start, end = 0, 0\n",
    "    for i in range(1, len(layers)):\n",
    "        # Reshape the section (start, end) and assign it to W[i]\n",
    "        end = start + layers[i - 1] * layers[i]\n",
    "        w[i] = theta[start: end].reshape(layers[i - 1], layers[i])\n",
    "        gw[i] = gtheta[start: end].reshape(layers[i - 1], layers[i])\n",
    "\n",
    "        # Reshape the section (start, end) and assign it to b[i]\n",
    "        start, end = end, end + layers[i]\n",
    "        b[i] = theta[start: end].reshape(layers[i])\n",
    "        gb[i] = gtheta[start: end].reshape(layers[i])\n",
    "        start = end\n",
    "    \n",
    "    return theta, gtheta, w, b, gw, gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef1e4e",
   "metadata": {},
   "source": [
    "#### Network class\n",
    "1. Forward Pass\n",
    "Z is the linear combination or pre-activations of a layer \n",
    "$$\n",
    "Z[l] = A[l-1] \\cdot w[l] + b[l]\n",
    "$$\n",
    "\n",
    "$A[l-1]$ is the activations of previous layer\n",
    "\n",
    "2. Backward Pass:\n",
    "Gradients of pre-activations Z wrt Loss are given by $ Z_{L}^{(g)} = \\hat{Y} - Y $ which is same for regression or classification. Similarly other gradients can be update iteratively by following formulas:<br>\n",
    "2.1 $ W_{L}^{(g)} =  A_{L-1}^{T} Z_{L}^{(g)}$<br><br>\n",
    "2.2 $ B_{L}^{(g)} = Z_{L}^{(g)}{^{T}}$<br><br>\n",
    "2.3 $ A_{L-1}^{(g)} = Z_{L}^{(g)} W_{L}^{T}$<br><br>\n",
    "2.4 $ Z_{L-1}^{(g)} = A_{L-1}^{(g)} \\odot g^{'}(Z_{l-1})$<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e00053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_regressor:\n",
    "    def __init__(self, layers, activation, out, loss_function):\n",
    "        self.layers = layers\n",
    "        self.theta, self.gtheta, self.w, self.b, self.gw, self.gb = init_params(layers)\n",
    "        \n",
    "        self.hid_act = act_func[activation]\n",
    "        self.grad_act = grad_act_func[activation]\n",
    "        self.out_layer_act = output_layer[out]\n",
    "        \n",
    "        self.loss = loss_func[loss_function]\n",
    "        \n",
    "    def forward(self,X):\n",
    "        self.z = [None for i in range(len(self.layers))]\n",
    "        self.a = [None for i in range(len(self.layers))]\n",
    "        \n",
    "        self.z[0] = X\n",
    "        self.a[0] = X\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.z[i] = (self.a[i-1] @ self.w[i]) + self.b[i]\n",
    "            self.a[i] = self.hid_act(self.z[i])\n",
    "            \n",
    "#             print(f\"Layer: {i} Preactivations: {self.z[i]} Activations: {self.a[i]}\\n\")\n",
    "            \n",
    "#         print(f\"Pre activation matrix: {self.z}\\nActivation Matrix: {self.a}\")\n",
    "        self.a[-1] = self.out_layer_act(self.z[-1])\n",
    "        \n",
    "        return self.a[-1]\n",
    "    \n",
    "    def backward(self,y,ypred):\n",
    "        gz = [None for i in range(len(self.layers))]\n",
    "        ga = [None for i in range(len(self.layers))]\n",
    "        gz[-1] = ypred - y\n",
    "        \n",
    "#         updating gradient from layer l-1 -> 0 (back-propagating)\n",
    "        for i in range(len(self.layers)-1, 0, -1):\n",
    "            self.gw[i][:, :] = self.a[i-1].T @ gz[i]\n",
    "            self.gb[i][:] = np.sum(gz[i].T,axis=1)\n",
    "            \n",
    "            ga[i-1] = gz[i] @ self.w[i].T\n",
    "            gz[i-1] = ga[i-1]*self.grad_act(self.z[i-1])\n",
    "            \n",
    "            \n",
    "    def fit(self, X, y, lr, epochs):\n",
    "        self.losses = []\n",
    "        self.accuracy_score = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "#             Forward Pass\n",
    "            ypred = self.forward(X)\n",
    "#             Loss \n",
    "            loss_val = self.loss(y,ypred)\n",
    "            acc_score = r2_score(y,ypred)\n",
    "            self.losses.append(loss_val)\n",
    "            self.accuracy_score.append(acc_score)\n",
    "            \n",
    "#             Backward pass\n",
    "            self.backward(y,ypred)\n",
    "    \n",
    "#             Update gradient\n",
    "            self.theta -= lr*self.gtheta\n",
    "    \n",
    "            if(i%10==0):\n",
    "                print(f\"Epoch:{i} Loss: {loss_val} Accuracy: {acc_score}\",end='\\n---------------------\\n')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ypred = self.forward(X)\n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98736339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_classifier:\n",
    "    def __init__(self, layers, activation, out, loss_function):\n",
    "        self.layers = layers\n",
    "        self.theta, self.gtheta, self.w, self.b, self.gw, self.gb = init_params(layers)\n",
    "        \n",
    "        self.hid_act = act_func[activation]\n",
    "        self.grad_act = grad_act_func[activation]\n",
    "        self.out_layer_act = output_layer[out]\n",
    "        \n",
    "        self.loss = loss_func[loss_function]\n",
    "        \n",
    "    def forward(self,X):\n",
    "        self.z = [None for i in range(len(self.layers))]\n",
    "        self.a = [None for i in range(len(self.layers))]\n",
    "        \n",
    "        self.z[0] = X\n",
    "        self.a[0] = X\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.z[i] = (self.a[i-1] @ self.w[i]) + self.b[i]\n",
    "            self.a[i] = self.hid_act(self.z[i])\n",
    "            \n",
    "#             print(f\"Layer: {i} Preactivations: {self.z[i]} Activations: {self.a[i]}\\n\")\n",
    "            \n",
    "#         print(f\"Pre activation matrix: {self.z}\\nActivation Matrix: {self.a}\")\n",
    "        self.a[-1] = self.out_layer_act(self.z[-1])\n",
    "        \n",
    "        return self.a[-1]\n",
    "    \n",
    "    def backward(self,y,ypred):\n",
    "        gz = [None for i in range(len(self.layers))]\n",
    "        ga = [None for i in range(len(self.layers))]\n",
    "        gz[-1] = ypred - y\n",
    "        \n",
    "#         updating gradient from layer l-1 -> 0 (back-propagating)\n",
    "        for i in range(len(self.layers)-1, 0, -1):\n",
    "            self.gw[i][:, :] = self.a[i-1].T @ gz[i]\n",
    "            self.gb[i][:] = np.sum(gz[i].T,axis=1)\n",
    "            \n",
    "            ga[i-1] = gz[i] @ self.w[i].T\n",
    "            gz[i-1] = ga[i-1]*self.grad_act(self.z[i-1])\n",
    "            \n",
    "            \n",
    "    def fit(self, X, y, lr, epochs):\n",
    "        self.losses = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "#             Forward Pass\n",
    "            ypred = self.forward(X)\n",
    "#             Loss \n",
    "            loss_val = self.loss(y,ypred)\n",
    "            self.losses.append(loss_val)\n",
    "            \n",
    "#             Backward pass\n",
    "            self.backward(y,ypred)\n",
    "    \n",
    "#             Update gradient\n",
    "            self.theta -= lr*self.gtheta\n",
    "    \n",
    "            if(i%10==0):\n",
    "                print(f\"Epoch:{i} Loss: {loss_val}\",end='\\n---------------------\\n')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ypred = self.forward(X)\n",
    "        return np.argmax(ypred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866ee4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a4dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
